{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spatio-Temporal Anomaly Detection in videos with Causal LSTM networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brngl/spatio-temporal-anomaly-detection-with-causalLSTM-networks/blob/master/Spatio_Temporal_Anomaly_Detection_in_videos_with_Causal_LSTM_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipGSpRE_s4wO",
        "colab_type": "text"
      },
      "source": [
        "# Spatio-Temporal Anomaly Detection in videos with Causal LSTM networks\n",
        "\n",
        "Here is a model based on Causal LSTM networks to make video anomaly detection. This model modifies [PredRNN++](https://github.com/Yunbo426/predrnn-pp) (GitHub link) work by Yunbo Wang et al. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Yunbo Wang et al. paper:\n",
        "[PredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning](https://arxiv.org/abs/1804.06300)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHPMPgWyBE-0",
        "colab_type": "code",
        "outputId": "2f28295a-638d-4a76-ba8e-24388754b422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Dec 22 19:12:13 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTPK75F_F13b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaUYCAVqiuiB",
        "colab_type": "text"
      },
      "source": [
        "### Variabili save/restore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFXKyi0HiHmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Set iterazione to 1 for a new execution.\n",
        "If you want to restore a checkpoint, set iterazione to checkpoint iteration + 1.\n",
        "For example, if checkpoint it's saved to iteration 1000, iterazione must be 1001.\n",
        "'''\n",
        "iterazione = 1\n",
        "model_checkpoint = 'checkpoints-pred/mnist_predrnn_pp/model.ckpt-'+str(iterazione-1)+'.index'\n",
        "model_checkpoint_meta = 'checkpoints-pred/mnist_predrnn_pp/model.ckpt-'+str(iterazione-1)+'.meta'\n",
        "if iterazione == 1:\n",
        "  model_checkpoint = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqSEl0WAindW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (model_checkpoint)\n",
        "print (model_checkpoint_meta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkzUdb8WvYO8",
        "colab_type": "text"
      },
      "source": [
        "### Google Drive Mounting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUtxTKutvcZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Mount Google Drive from your account and move in predrnn-pp-vad directory.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiLRgAtZWTeu",
        "colab_type": "text"
      },
      "source": [
        "### Parameters definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKrbmCXIWjVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import sys\n",
        "import random\n",
        "from nets import models_factory\n",
        "from data_provider import datasets_factory\n",
        "from utils import preprocess\n",
        "from utils import metrics\n",
        "from skimage.measure import compare_ssim\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel') #Per risolvere un bug\n",
        "\n",
        "# data I/O\n",
        "tf.app.flags.DEFINE_string('dataset_name', 'mnist',\n",
        "                           'The name of dataset.')\n",
        "tf.app.flags.DEFINE_string('train_data_paths',\n",
        "                           'data/ano-moving-mnist/new_training-set.npz', #anom_train.npz\n",
        "                           'train data paths.')\n",
        "tf.app.flags.DEFINE_string('valid_data_paths',\n",
        "                           'data/ano-moving-mnist/anom_valid.npz',\n",
        "                           'validation data paths.')\n",
        "tf.app.flags.DEFINE_string('save_dir', 'checkpoints-pred/mnist_predrnn_pp',\n",
        "                            'dir to store trained net.')\n",
        "tf.app.flags.DEFINE_string('gen_frm_dir', 'results-pred/mnist_predrnn_pp',\n",
        "                           'dir to store result.')\n",
        "# model\n",
        "tf.app.flags.DEFINE_string('model_name', 'predrnn_pp',\n",
        "                           'The name of the architecture.')\n",
        "tf.app.flags.DEFINE_string('pretrained_model', model_checkpoint, #checkpoints/mnist_predrnn_pp/model.ckpt-10.index\n",
        "                           'file of a pretrained model to initialize from.')\n",
        "tf.app.flags.DEFINE_integer('input_length', 10,\n",
        "                            'encoder hidden states.')\n",
        "tf.app.flags.DEFINE_integer('seq_length', 20,\n",
        "                            'total input and output length.')\n",
        "tf.app.flags.DEFINE_integer('img_width', 64,\n",
        "                            'input image width.')\n",
        "tf.app.flags.DEFINE_integer('img_channel', 1,\n",
        "                            'number of image channel.')\n",
        "tf.app.flags.DEFINE_integer('stride', 1,\n",
        "                            'stride of a convlstm layer.')\n",
        "tf.app.flags.DEFINE_integer('filter_size', 5, #5\n",
        "                            'filter of a convlstm layer.')\n",
        "tf.app.flags.DEFINE_string('num_hidden', '128,64,64,64',\n",
        "                           'COMMA separated number of units in a convlstm layer.') #128,64,64,64\n",
        "tf.app.flags.DEFINE_integer('patch_size', 4,\n",
        "                            'patch size on one dimension.')\n",
        "tf.app.flags.DEFINE_boolean('layer_norm', True,\n",
        "                            'whether to apply tensor layer norm.')\n",
        "# optimization\n",
        "tf.app.flags.DEFINE_float('lr', 0.001,\n",
        "                          'base learning rate.')\n",
        "tf.app.flags.DEFINE_boolean('reverse_input', True,\n",
        "                            'whether to reverse the input frames while training.')\n",
        "tf.app.flags.DEFINE_integer('batch_size', 8,\n",
        "                            'batch size for training.')\n",
        "tf.app.flags.DEFINE_integer('max_iterations', 80000, # 80000\n",
        "                            'max num of steps.')\n",
        "tf.app.flags.DEFINE_integer('display_interval', 1,\n",
        "                            'number of iters showing training loss.')\n",
        "tf.app.flags.DEFINE_integer('test_interval', 10000,# 2000\n",
        "                            'number of iters for test.')\n",
        "tf.app.flags.DEFINE_integer('snapshot_interval', 500, #10000\n",
        "                            'number of iters saving models.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2iAByHyWmwn",
        "colab_type": "text"
      },
      "source": [
        "### Definition of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRjtANBdWtpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inputs\n",
        "s_x = tf.placeholder(tf.float32,\n",
        "                        [FLAGS.batch_size,\n",
        "                         FLAGS.seq_length,\n",
        "                         FLAGS.img_width / FLAGS.patch_size,\n",
        "                         FLAGS.img_width / FLAGS.patch_size,\n",
        "                         FLAGS.patch_size * FLAGS.patch_size * FLAGS.img_channel])\n",
        "\n",
        "s_mask_true = tf.placeholder(tf.float32,\n",
        "                                [FLAGS.batch_size,\n",
        "                                 FLAGS.seq_length - FLAGS.input_length - 1,\n",
        "                                 FLAGS.img_width / FLAGS.patch_size,\n",
        "                                 FLAGS.img_width / FLAGS.patch_size,\n",
        "                                 FLAGS.patch_size * FLAGS.patch_size * FLAGS.img_channel])\n",
        "\n",
        "grads = []\n",
        "loss_train = []\n",
        "s_pred_seq = []\n",
        "s_tf_lr = tf.placeholder(tf.float32, shape=[])\n",
        "num_hidden = [int(x) for x in FLAGS.num_hidden.split(',')]\n",
        "print('Num. Hidden: ', num_hidden)\n",
        "num_layers = len(num_hidden)\n",
        "with tf.variable_scope(tf.get_variable_scope()):\n",
        "    # define a model\n",
        "    output_list = models_factory.construct_model(\n",
        "        FLAGS.model_name, s_x,\n",
        "        s_mask_true,\n",
        "        num_layers, num_hidden,\n",
        "        FLAGS.filter_size, FLAGS.stride,\n",
        "        FLAGS.seq_length, FLAGS.input_length,\n",
        "        FLAGS.layer_norm)\n",
        "    gen_ims = output_list[0] \n",
        "    loss = output_list[1]\n",
        "    pred_ims = gen_ims[:, FLAGS.input_length - 1:]\n",
        "    s_loss_train = loss / FLAGS.batch_size\n",
        "    # gradients\n",
        "    all_params = tf.trainable_variables()\n",
        "    grads.append(tf.gradients(loss, all_params))\n",
        "    s_pred_seq.append(pred_ims)\n",
        "\n",
        "s_train_op = tf.train.AdamOptimizer(FLAGS.lr).minimize(loss)\n",
        "\n",
        "# session\n",
        "variables = tf.global_variables()\n",
        "s_saver = tf.train.Saver(variables)\n",
        "init = tf.global_variables_initializer()\n",
        "configProt = tf.ConfigProto()\n",
        "configProt.gpu_options.allow_growth = True\n",
        "configProt.allow_soft_placement = True\n",
        "s_sess = tf.Session(config=configProt)\n",
        "s_sess.run(init)\n",
        "print(\"C'è un pretrained model?\")\n",
        "print(FLAGS.pretrained_model)\n",
        "if FLAGS.pretrained_model:\n",
        "    print(\"Sto facendo il restore...\")\n",
        "    tf.reset_default_graph()\n",
        "    s_saver = tf.train.import_meta_graph(model_checkpoint_meta)\n",
        "    s_saver.restore(s_sess, tf.train.latest_checkpoint('checkpoints-pred/mnist_predrnn_pp/'))\n",
        "    print('...restore completato')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzPh2xxtYtzi",
        "colab_type": "text"
      },
      "source": [
        "### Functions definition for:\n",
        "\n",
        "- training\n",
        "- testing\n",
        "- checkpoint saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8KOXICpYo-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(inputs, lr, mask_true):\n",
        "    feed_dict = {s_x: inputs}\n",
        "    feed_dict.update({s_tf_lr: lr})\n",
        "    feed_dict.update({s_mask_true: mask_true})\n",
        "    loss, _ = s_sess.run((s_loss_train, s_train_op), feed_dict)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def test(inputs, mask_true):\n",
        "    feed_dict = {s_x: inputs}\n",
        "    feed_dict.update({s_mask_true: mask_true})\n",
        "    gen_ims = s_sess.run(s_pred_seq, feed_dict)\n",
        "    return gen_ims\n",
        "\n",
        "\n",
        "def save(itr):\n",
        "    checkpoint_path = os.path.join(FLAGS.save_dir, 'model.ckpt')\n",
        "    s_saver.save(s_sess, checkpoint_path, global_step=itr)\n",
        "    print('saved to ' + FLAGS.save_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_DhFGJYZLYZ",
        "colab_type": "text"
      },
      "source": [
        "### Directory management\n",
        "Manages checkpoint and results directories.\n",
        "\n",
        "At the first iteration, if the directories exist yet, delete and recreate them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEn2i567Y-gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if tf.gfile.Exists(FLAGS.save_dir) and iterazione == 1 :\n",
        "    tf.gfile.DeleteRecursively(FLAGS.save_dir)\n",
        "tf.gfile.MakeDirs(FLAGS.save_dir)\n",
        "\n",
        "if iterazione == 1:\n",
        "  if tf.gfile.Exists(FLAGS.gen_frm_dir):\n",
        "    tf.gfile.DeleteRecursively(FLAGS.gen_frm_dir)\n",
        "  tf.gfile.MakeDirs(FLAGS.gen_frm_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuhyhmQ0ZtKu",
        "colab_type": "text"
      },
      "source": [
        "### Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoreiyMXZnMO",
        "colab_type": "code",
        "outputId": "c06342f0-43ac-4d8a-de42-9e0be33333df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "# load data\n",
        "train_input_handle, test_input_handle = datasets_factory.data_provider(\n",
        "    FLAGS.dataset_name, FLAGS.train_data_paths, FLAGS.valid_data_paths,\n",
        "    FLAGS.batch_size, FLAGS.img_width)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clips\n",
            "(2, 3000, 2)\n",
            "dims\n",
            "(1, 3)\n",
            "input_raw_data\n",
            "(60000, 1, 64, 64)\n",
            "clips\n",
            "(2, 10000, 2)\n",
            "dims\n",
            "(1, 3)\n",
            "input_raw_data\n",
            "(200000, 1, 64, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu4yuSrbaz7g",
        "colab_type": "text"
      },
      "source": [
        "These parameters are used for training scheduling. *eta* value decreases at every iteration of a value equal to *delta* until it reaches zero value at the 50000th iteration. \n",
        "Every time a checkpoint is restored, *eta* must have the right value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5cvklJPbUu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = FLAGS.lr\n",
        "\n",
        "delta = 0.00002\n",
        "base = 0.99998\n",
        "#eta = etaaa #1\n",
        "eta = 1\n",
        "if(iterazione > 1):\n",
        "  f = open(\"eta.txt\", \"r\")\n",
        "  eta = f.read()\n",
        "  eta = float(eta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-06_uFibq5O",
        "colab_type": "text"
      },
      "source": [
        "### Training phase\n",
        "At every *FLAGS.test_interval* iterations, it's executed a validation step. So, the predicted images are saved and are mesured these metrics:\n",
        "- mse\n",
        "- ssim\n",
        "- psnr\n",
        "- fmae\n",
        "- sharpness\n",
        "\n",
        "The generated images are saved in *results* directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GPCCaj8dfeZ",
        "colab_type": "code",
        "outputId": "bae99bd9-04f4-4456-c959-84d0c94790eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for itr in xrange(iterazione, FLAGS.max_iterations + 1):\n",
        "    if train_input_handle.no_batch_left():\n",
        "        train_input_handle.begin(do_shuffle=True)\n",
        "    ims = train_input_handle.get_batch() #Ritorna input_batch + output_batch concatenati\n",
        "    ims = preprocess.reshape_patch(ims, FLAGS.patch_size) #patch_size = 4. Restituisce un array (8, 10, 16, 16, 16)\n",
        "\n",
        "    if itr < 50000:\n",
        "        eta -= delta  #50000 * 0.00002 = 1\n",
        "        eta = float('%.5f'%(eta))\n",
        "        print(\"eta\", eta)\n",
        "    else:\n",
        "        eta = 0.0\n",
        "    random_flip = np.random.random_sample(\n",
        "        (FLAGS.batch_size, FLAGS.seq_length - FLAGS.input_length - 1)) #dim (8, 4)\n",
        "    true_token = (random_flip < eta)\n",
        "    # true_token = (random_flip < pow(base,itr))\n",
        "    ones = np.ones((FLAGS.img_width / FLAGS.patch_size,\n",
        "                    FLAGS.img_width / FLAGS.patch_size,\n",
        "                    FLAGS.patch_size ** 2 * FLAGS.img_channel)) #dim (16, 16, 16)\n",
        "    zeros = np.zeros((FLAGS.img_width / FLAGS.patch_size,\n",
        "                      FLAGS.img_width / FLAGS.patch_size,\n",
        "                      FLAGS.patch_size ** 2 * FLAGS.img_channel)) #dim (16, 16, 16)\n",
        "    mask_true = []\n",
        "    for i in xrange(FLAGS.batch_size):\n",
        "        for j in xrange(FLAGS.seq_length - FLAGS.input_length - 1):\n",
        "            if true_token[i, j]:\n",
        "                mask_true.append(ones)\n",
        "            else:\n",
        "                mask_true.append(zeros)\n",
        "    mask_true = np.array(mask_true)\n",
        "    #print('Mask true shape', mask_true.shape)\n",
        "    mask_true = np.reshape(mask_true, (FLAGS.batch_size,\n",
        "                                       FLAGS.seq_length - FLAGS.input_length - 1,\n",
        "                                       FLAGS.img_width / FLAGS.patch_size,\n",
        "                                       FLAGS.img_width / FLAGS.patch_size,\n",
        "                                       FLAGS.patch_size ** 2 * FLAGS.img_channel))\n",
        "    #print('Mask true dopo reshape', mask_true.shape)\n",
        "    #TRAINING\n",
        "    cost = train(ims, lr, mask_true)\n",
        "    if FLAGS.reverse_input:\n",
        "        ims_rev = ims[:, ::-1]\n",
        "        cost += train(ims_rev, lr, mask_true)\n",
        "        cost = cost / 2\n",
        "\n",
        "    if itr % FLAGS.display_interval == 0:\n",
        "        print('itr: ' + str(itr))\n",
        "        print('training loss: ' + str(cost))\n",
        "\n",
        "    if itr % FLAGS.test_interval == 0:\n",
        "        print('test...')\n",
        "        test_input_handle.begin(do_shuffle=False)\n",
        "        res_path = os.path.join(FLAGS.gen_frm_dir, str(itr))\n",
        "        os.mkdir(res_path)\n",
        "        avg_mse = 0\n",
        "        batch_id = 0\n",
        "        img_mse, ssim, psnr, fmae, sharp = [], [], [], [], []\n",
        "        for i in xrange(FLAGS.seq_length - FLAGS.input_length):\n",
        "            img_mse.append(0)\n",
        "            ssim.append(0)\n",
        "            psnr.append(0)\n",
        "            fmae.append(0)\n",
        "            sharp.append(0)\n",
        "        mask_true = np.zeros((FLAGS.batch_size,\n",
        "                              FLAGS.seq_length - FLAGS.input_length - 1,\n",
        "                              FLAGS.img_width / FLAGS.patch_size,\n",
        "                              FLAGS.img_width / FLAGS.patch_size,\n",
        "                              FLAGS.patch_size ** 2 * FLAGS.img_channel))\n",
        "        while (test_input_handle.no_batch_left() == False):\n",
        "            batch_id = batch_id + 1\n",
        "            test_ims = test_input_handle.get_batch()\n",
        "            test_dat = preprocess.reshape_patch(test_ims, FLAGS.patch_size)\n",
        "            img_gen = test(test_dat, mask_true)\n",
        "            # concat outputs of different gpus along batch\n",
        "            img_gen = np.concatenate(img_gen)\n",
        "            img_gen = preprocess.reshape_patch_back(img_gen, FLAGS.patch_size)\n",
        "            # MSE per frame\n",
        "            for i in xrange(FLAGS.seq_length - FLAGS.input_length):\n",
        "                x = test_ims[:, i + FLAGS.input_length, :, :, 0]\n",
        "                gx = img_gen[:, i, :, :, 0]\n",
        "                fmae[i] += metrics.batch_mae_frame_float(gx, x)\n",
        "                gx = np.maximum(gx, 0)\n",
        "                gx = np.minimum(gx, 1)\n",
        "                mse = np.square(x - gx).sum()\n",
        "                img_mse[i] += mse\n",
        "                avg_mse += mse\n",
        "\n",
        "                real_frm = np.uint8(x * 255)\n",
        "                pred_frm = np.uint8(gx * 255)\n",
        "                psnr[i] += metrics.batch_psnr(pred_frm, real_frm)\n",
        "                for b in xrange(FLAGS.batch_size):\n",
        "                    sharp[i] += np.max(\n",
        "                        cv2.convertScaleAbs(cv2.Laplacian(pred_frm[b], 3)))\n",
        "                    score, _ = compare_ssim(pred_frm[b], real_frm[b], full=True)\n",
        "                    ssim[i] += score\n",
        "            # save prediction examples\n",
        "            if batch_id <= 10:\n",
        "                path = os.path.join(res_path, str(batch_id))\n",
        "                os.mkdir(path)\n",
        "                for i in xrange(FLAGS.seq_length):\n",
        "                    name = 'gt' + str(i + 1) + '.png'\n",
        "                    file_name = os.path.join(path, name)\n",
        "                    img_gt = np.uint8(test_ims[0, i, :, :, :] * 255)\n",
        "                    cv2.imwrite(file_name, img_gt)\n",
        "                for i in xrange(FLAGS.seq_length - FLAGS.input_length):\n",
        "                    name = 'pd' + str(i + 1 + FLAGS.input_length) + '.png'\n",
        "                    file_name = os.path.join(path, name)\n",
        "                    img_pd = img_gen[0, i, :, :, :]\n",
        "                    img_pd = np.maximum(img_pd, 0)\n",
        "                    img_pd = np.minimum(img_pd, 1)\n",
        "                    img_pd = np.uint8(img_pd * 255)\n",
        "                    cv2.imwrite(file_name, img_pd)\n",
        "            test_input_handle.next()\n",
        "        avg_mse = avg_mse / (batch_id * FLAGS.batch_size)\n",
        "        print('mse per seq: ' + str(avg_mse))\n",
        "        for i in xrange(FLAGS.seq_length - FLAGS.input_length):\n",
        "            print(img_mse[i] / (batch_id * FLAGS.batch_size))\n",
        "        psnr = np.asarray(psnr, dtype=np.float32) / batch_id\n",
        "        fmae = np.asarray(fmae, dtype=np.float32) / batch_id\n",
        "        ssim = np.asarray(ssim, dtype=np.float32) / (FLAGS.batch_size * batch_id)\n",
        "        sharp = np.asarray(sharp, dtype=np.float32) / (FLAGS.batch_size * batch_id)\n",
        "        print('psnr per frame: ' + str(np.mean(psnr)))\n",
        "        for i in xrange(FLAGS.seq_length - FLAGS.input_length):\n",
        "            print(psnr[i])\n",
        "        print('fmae per frame: ' + str(np.mean(fmae)))\n",
        "        for i in xrange(FLAGS.seq_length - FLAGS.input_length):\n",
        "            print(fmae[i])\n",
        "        print('ssim per frame: ' + str(np.mean(ssim)))\n",
        "        for i in xrange(FLAGS.seq_length - FLAGS.input_length):\n",
        "            print(ssim[i])\n",
        "        print('sharpness per frame: ' + str(np.mean(sharp)))\n",
        "        for i in xrange(FLAGS.seq_length - FLAGS.input_length):\n",
        "            print(sharp[i])\n",
        "    \n",
        "    if itr % FLAGS.snapshot_interval == 0:\n",
        "      f = open(\"eta.txt\", \"w+\")\n",
        "      f.write(str(eta))\n",
        "      f.close()\n",
        "      if tf.gfile.Exists(FLAGS.save_dir):\n",
        "        tf.gfile.DeleteRecursively(FLAGS.save_dir)\n",
        "      tf.gfile.MakeDirs(FLAGS.save_dir)\n",
        "      save(itr)\n",
        "    \n",
        "    train_input_handle.next()\n",
        "\n",
        "print(\"FINITO! :)\")    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "itr: 74606\n",
            "training loss: 58.50434494018555\n",
            "itr: 74607\n",
            "training loss: 52.47566223144531\n",
            "itr: 74608\n",
            "training loss: 45.26495361328125\n",
            "itr: 74609\n",
            "training loss: 54.099266052246094\n",
            "itr: 74610\n",
            "training loss: 47.841529846191406\n",
            "itr: 74611\n",
            "training loss: 54.285377502441406\n",
            "itr: 74612\n",
            "training loss: 55.52727508544922\n",
            "itr: 74613\n",
            "training loss: 51.817230224609375\n",
            "itr: 74614\n",
            "training loss: 46.146751403808594\n",
            "itr: 74615\n",
            "training loss: 46.17295837402344\n",
            "itr: 74616\n",
            "training loss: 60.316253662109375\n",
            "itr: 74617\n",
            "training loss: 50.674072265625\n",
            "itr: 74618\n",
            "training loss: 52.96582794189453\n",
            "itr: 74619\n",
            "training loss: 52.51131820678711\n",
            "itr: 74620\n",
            "training loss: 51.35956573486328\n",
            "itr: 74621\n",
            "training loss: 50.101234436035156\n",
            "itr: 74622\n",
            "training loss: 56.71583938598633\n",
            "itr: 74623\n",
            "training loss: 55.41116714477539\n",
            "itr: 74624\n",
            "training loss: 55.684410095214844\n",
            "itr: 74625\n",
            "training loss: 55.32701873779297\n",
            "itr: 74626\n",
            "training loss: 59.1476936340332\n",
            "itr: 74627\n",
            "training loss: 50.93190383911133\n",
            "itr: 74628\n",
            "training loss: 54.239097595214844\n",
            "itr: 74629\n",
            "training loss: 50.95655059814453\n",
            "itr: 74630\n",
            "training loss: 51.058860778808594\n",
            "itr: 74631\n",
            "training loss: 48.12059783935547\n",
            "itr: 74632\n",
            "training loss: 48.81929016113281\n",
            "itr: 74633\n",
            "training loss: 48.21638488769531\n",
            "itr: 74634\n",
            "training loss: 47.344825744628906\n",
            "itr: 74635\n",
            "training loss: 49.472286224365234\n",
            "itr: 74636\n",
            "training loss: 49.47623062133789\n",
            "itr: 74637\n",
            "training loss: 46.21046447753906\n",
            "itr: 74638\n",
            "training loss: 58.23649597167969\n",
            "itr: 74639\n",
            "training loss: 52.97454833984375\n",
            "itr: 74640\n",
            "training loss: 56.2374267578125\n",
            "itr: 74641\n",
            "training loss: 46.48834991455078\n",
            "itr: 74642\n",
            "training loss: 53.018436431884766\n",
            "itr: 74643\n",
            "training loss: 44.63603210449219\n",
            "itr: 74644\n",
            "training loss: 47.500877380371094\n",
            "itr: 74645\n",
            "training loss: 54.11210250854492\n",
            "itr: 74646\n",
            "training loss: 48.80792236328125\n",
            "itr: 74647\n",
            "training loss: 54.61280822753906\n",
            "itr: 74648\n",
            "training loss: 57.65566635131836\n",
            "itr: 74649\n",
            "training loss: 52.2820930480957\n",
            "itr: 74650\n",
            "training loss: 55.57727813720703\n",
            "itr: 74651\n",
            "training loss: 49.34187316894531\n",
            "itr: 74652\n",
            "training loss: 50.438026428222656\n",
            "itr: 74653\n",
            "training loss: 46.49908447265625\n",
            "itr: 74654\n",
            "training loss: 47.867679595947266\n",
            "itr: 74655\n",
            "training loss: 47.276893615722656\n",
            "itr: 74656\n",
            "training loss: 52.22655487060547\n",
            "itr: 74657\n",
            "training loss: 44.307579040527344\n",
            "itr: 74658\n",
            "training loss: 53.06522750854492\n",
            "itr: 74659\n",
            "training loss: 60.23359680175781\n",
            "itr: 74660\n",
            "training loss: 63.38019561767578\n",
            "itr: 74661\n",
            "training loss: 53.01854705810547\n",
            "itr: 74662\n",
            "training loss: 49.487335205078125\n",
            "itr: 74663\n",
            "training loss: 49.93446731567383\n",
            "itr: 74664\n",
            "training loss: 56.726558685302734\n",
            "itr: 74665\n",
            "training loss: 46.28813171386719\n",
            "itr: 74666\n",
            "training loss: 54.12303924560547\n",
            "itr: 74667\n",
            "training loss: 60.978912353515625\n",
            "itr: 74668\n",
            "training loss: 47.381893157958984\n",
            "itr: 74669\n",
            "training loss: 52.926822662353516\n",
            "itr: 74670\n",
            "training loss: 50.694419860839844\n",
            "itr: 74671\n",
            "training loss: 56.81474304199219\n",
            "itr: 74672\n",
            "training loss: 56.22541046142578\n",
            "itr: 74673\n",
            "training loss: 51.853309631347656\n",
            "itr: 74674\n",
            "training loss: 58.801109313964844\n",
            "itr: 74675\n",
            "training loss: 54.43946838378906\n",
            "itr: 74676\n",
            "training loss: 51.5892219543457\n",
            "itr: 74677\n",
            "training loss: 49.1426887512207\n",
            "itr: 74678\n",
            "training loss: 56.87741470336914\n",
            "itr: 74679\n",
            "training loss: 48.22962188720703\n",
            "itr: 74680\n",
            "training loss: 59.827178955078125\n",
            "itr: 74681\n",
            "training loss: 55.22374725341797\n",
            "itr: 74682\n",
            "training loss: 55.082794189453125\n",
            "itr: 74683\n",
            "training loss: 56.14029312133789\n",
            "itr: 74684\n",
            "training loss: 52.68121337890625\n",
            "itr: 74685\n",
            "training loss: 49.820716857910156\n",
            "itr: 74686\n",
            "training loss: 48.75532913208008\n",
            "itr: 74687\n",
            "training loss: 51.402618408203125\n",
            "itr: 74688\n",
            "training loss: 51.059288024902344\n",
            "itr: 74689\n",
            "training loss: 47.816158294677734\n",
            "itr: 74690\n",
            "training loss: 51.66555404663086\n",
            "itr: 74691\n",
            "training loss: 45.865570068359375\n",
            "itr: 74692\n",
            "training loss: 54.85865783691406\n",
            "itr: 74693\n",
            "training loss: 47.476104736328125\n",
            "itr: 74694\n",
            "training loss: 58.81117248535156\n",
            "itr: 74695\n",
            "training loss: 48.35057067871094\n",
            "itr: 74696\n",
            "training loss: 49.0941162109375\n",
            "itr: 74697\n",
            "training loss: 45.06055450439453\n",
            "itr: 74698\n",
            "training loss: 58.201904296875\n",
            "itr: 74699\n",
            "training loss: 50.256622314453125\n",
            "itr: 74700\n",
            "training loss: 54.26891326904297\n",
            "itr: 74701\n",
            "training loss: 49.50342559814453\n",
            "itr: 74702\n",
            "training loss: 44.646419525146484\n",
            "itr: 74703\n",
            "training loss: 54.02600860595703\n",
            "itr: 74704\n",
            "training loss: 47.79194641113281\n",
            "itr: 74705\n",
            "training loss: 53.06752014160156\n",
            "itr: 74706\n",
            "training loss: 57.62846374511719\n",
            "itr: 74707\n",
            "training loss: 51.59017562866211\n",
            "itr: 74708\n",
            "training loss: 49.555362701416016\n",
            "itr: 74709\n",
            "training loss: 50.861656188964844\n",
            "itr: 74710\n",
            "training loss: 47.348350524902344\n",
            "itr: 74711\n",
            "training loss: 54.22529983520508\n",
            "itr: 74712\n",
            "training loss: 48.601409912109375\n",
            "itr: 74713\n",
            "training loss: 48.44239044189453\n",
            "itr: 74714\n",
            "training loss: 50.374908447265625\n",
            "itr: 74715\n",
            "training loss: 48.02379608154297\n",
            "itr: 74716\n",
            "training loss: 50.183197021484375\n",
            "itr: 74717\n",
            "training loss: 53.6746940612793\n",
            "itr: 74718\n",
            "training loss: 49.61217498779297\n",
            "itr: 74719\n",
            "training loss: 56.89960479736328\n",
            "itr: 74720\n",
            "training loss: 52.54864501953125\n",
            "itr: 74721\n",
            "training loss: 60.308197021484375\n",
            "itr: 74722\n",
            "training loss: 47.63962173461914\n",
            "itr: 74723\n",
            "training loss: 50.1955451965332\n",
            "itr: 74724\n",
            "training loss: 55.588172912597656\n",
            "itr: 74725\n",
            "training loss: 51.44523620605469\n",
            "itr: 74726\n",
            "training loss: 51.50366973876953\n",
            "itr: 74727\n",
            "training loss: 56.63835144042969\n",
            "itr: 74728\n",
            "training loss: 51.4943962097168\n",
            "itr: 74729\n",
            "training loss: 55.54376983642578\n",
            "itr: 74730\n",
            "training loss: 53.43180847167969\n",
            "itr: 74731\n",
            "training loss: 58.66131591796875\n",
            "itr: 74732\n",
            "training loss: 54.32126998901367\n",
            "itr: 74733\n",
            "training loss: 56.79271697998047\n",
            "itr: 74734\n",
            "training loss: 57.84996032714844\n",
            "itr: 74735\n",
            "training loss: 52.074119567871094\n",
            "itr: 74736\n",
            "training loss: 54.22056579589844\n",
            "itr: 74737\n",
            "training loss: 62.459327697753906\n",
            "itr: 74738\n",
            "training loss: 50.239627838134766\n",
            "itr: 74739\n",
            "training loss: 52.40119934082031\n",
            "itr: 74740\n",
            "training loss: 49.99458312988281\n",
            "itr: 74741\n",
            "training loss: 51.99101257324219\n",
            "itr: 74742\n",
            "training loss: 54.483497619628906\n",
            "itr: 74743\n",
            "training loss: 47.639095306396484\n",
            "itr: 74744\n",
            "training loss: 57.015724182128906\n",
            "itr: 74745\n",
            "training loss: 57.01343536376953\n",
            "itr: 74746\n",
            "training loss: 50.34999084472656\n",
            "itr: 74747\n",
            "training loss: 47.461448669433594\n",
            "itr: 74748\n",
            "training loss: 47.956756591796875\n",
            "itr: 74749\n",
            "training loss: 57.66988754272461\n",
            "itr: 74750\n",
            "training loss: 55.38923263549805\n",
            "itr: 74751\n",
            "training loss: 55.93628692626953\n",
            "itr: 74752\n",
            "training loss: 59.84939956665039\n",
            "itr: 74753\n",
            "training loss: 54.12919998168945\n",
            "itr: 74754\n",
            "training loss: 55.18950653076172\n",
            "itr: 74755\n",
            "training loss: 60.74163818359375\n",
            "itr: 74756\n",
            "training loss: 53.51908874511719\n",
            "itr: 74757\n",
            "training loss: 51.71921920776367\n",
            "itr: 74758\n",
            "training loss: 46.94657516479492\n",
            "itr: 74759\n",
            "training loss: 50.008544921875\n",
            "itr: 74760\n",
            "training loss: 53.33439254760742\n",
            "itr: 74761\n",
            "training loss: 49.57907485961914\n",
            "itr: 74762\n",
            "training loss: 53.88254928588867\n",
            "itr: 74763\n",
            "training loss: 49.210205078125\n",
            "itr: 74764\n",
            "training loss: 51.331512451171875\n",
            "itr: 74765\n",
            "training loss: 54.74128341674805\n",
            "itr: 74766\n",
            "training loss: 52.06055450439453\n",
            "itr: 74767\n",
            "training loss: 56.325531005859375\n",
            "itr: 74768\n",
            "training loss: 52.87849426269531\n",
            "itr: 74769\n",
            "training loss: 59.602012634277344\n",
            "itr: 74770\n",
            "training loss: 49.66167449951172\n",
            "itr: 74771\n",
            "training loss: 55.021724700927734\n",
            "itr: 74772\n",
            "training loss: 59.942691802978516\n",
            "itr: 74773\n",
            "training loss: 46.13508605957031\n",
            "itr: 74774\n",
            "training loss: 57.539180755615234\n",
            "itr: 74775\n",
            "training loss: 48.256805419921875\n",
            "itr: 74776\n",
            "training loss: 60.929256439208984\n",
            "itr: 74777\n",
            "training loss: 58.48833465576172\n",
            "itr: 74778\n",
            "training loss: 56.473175048828125\n",
            "itr: 74779\n",
            "training loss: 56.81886291503906\n",
            "itr: 74780\n",
            "training loss: 53.11854553222656\n",
            "itr: 74781\n",
            "training loss: 54.53810501098633\n",
            "itr: 74782\n",
            "training loss: 47.577232360839844\n",
            "itr: 74783\n",
            "training loss: 49.02454376220703\n",
            "itr: 74784\n",
            "training loss: 59.32978057861328\n",
            "itr: 74785\n",
            "training loss: 51.94618606567383\n",
            "itr: 74786\n",
            "training loss: 55.847816467285156\n",
            "itr: 74787\n",
            "training loss: 52.140625\n",
            "itr: 74788\n",
            "training loss: 54.49733352661133\n",
            "itr: 74789\n",
            "training loss: 53.796939849853516\n",
            "itr: 74790\n",
            "training loss: 53.95441436767578\n",
            "itr: 74791\n",
            "training loss: 49.07596206665039\n",
            "itr: 74792\n",
            "training loss: 46.04319763183594\n",
            "itr: 74793\n",
            "training loss: 50.63653564453125\n",
            "itr: 74794\n",
            "training loss: 55.454559326171875\n",
            "itr: 74795\n",
            "training loss: 57.32343292236328\n",
            "itr: 74796\n",
            "training loss: 54.42115020751953\n",
            "itr: 74797\n",
            "training loss: 49.76245880126953\n",
            "itr: 74798\n",
            "training loss: 51.07307815551758\n",
            "itr: 74799\n",
            "training loss: 59.366798400878906\n",
            "itr: 74800\n",
            "training loss: 53.107276916503906\n",
            "itr: 74801\n",
            "training loss: 52.34641647338867\n",
            "itr: 74802\n",
            "training loss: 50.12864685058594\n",
            "itr: 74803\n",
            "training loss: 53.03126525878906\n",
            "itr: 74804\n",
            "training loss: 52.11094665527344\n",
            "itr: 74805\n",
            "training loss: 57.93318557739258\n",
            "itr: 74806\n",
            "training loss: 54.09967803955078\n",
            "itr: 74807\n",
            "training loss: 48.63994598388672\n",
            "itr: 74808\n",
            "training loss: 46.97187423706055\n",
            "itr: 74809\n",
            "training loss: 61.25535583496094\n",
            "itr: 74810\n",
            "training loss: 53.89714050292969\n",
            "itr: 74811\n",
            "training loss: 52.215850830078125\n",
            "itr: 74812\n",
            "training loss: 46.6732177734375\n",
            "itr: 74813\n",
            "training loss: 53.56049728393555\n",
            "itr: 74814\n",
            "training loss: 58.227989196777344\n",
            "itr: 74815\n",
            "training loss: 48.90003204345703\n",
            "itr: 74816\n",
            "training loss: 51.39312744140625\n",
            "itr: 74817\n",
            "training loss: 61.53416442871094\n",
            "itr: 74818\n",
            "training loss: 49.245094299316406\n",
            "itr: 74819\n",
            "training loss: 46.84400177001953\n",
            "itr: 74820\n",
            "training loss: 53.878108978271484\n",
            "itr: 74821\n",
            "training loss: 52.37800979614258\n",
            "itr: 74822\n",
            "training loss: 50.61557388305664\n",
            "itr: 74823\n",
            "training loss: 59.89586639404297\n",
            "itr: 74824\n",
            "training loss: 49.34900665283203\n",
            "itr: 74825\n",
            "training loss: 48.65354919433594\n",
            "itr: 74826\n",
            "training loss: 54.12901306152344\n",
            "itr: 74827\n",
            "training loss: 50.231658935546875\n",
            "itr: 74828\n",
            "training loss: 53.249298095703125\n",
            "itr: 74829\n",
            "training loss: 50.079769134521484\n",
            "itr: 74830\n",
            "training loss: 49.968101501464844\n",
            "itr: 74831\n",
            "training loss: 57.67955017089844\n",
            "itr: 74832\n",
            "training loss: 49.15964126586914\n",
            "itr: 74833\n",
            "training loss: 54.67578887939453\n",
            "itr: 74834\n",
            "training loss: 54.13008117675781\n",
            "itr: 74835\n",
            "training loss: 46.23918151855469\n",
            "itr: 74836\n",
            "training loss: 55.194664001464844\n",
            "itr: 74837\n",
            "training loss: 51.347511291503906\n",
            "itr: 74838\n",
            "training loss: 50.37002182006836\n",
            "itr: 74839\n",
            "training loss: 47.91863250732422\n",
            "itr: 74840\n",
            "training loss: 52.01069641113281\n",
            "itr: 74841\n",
            "training loss: 52.17191696166992\n",
            "itr: 74842\n",
            "training loss: 56.40204620361328\n",
            "itr: 74843\n",
            "training loss: 49.33972930908203\n",
            "itr: 74844\n",
            "training loss: 57.18195343017578\n",
            "itr: 74845\n",
            "training loss: 51.275230407714844\n",
            "itr: 74846\n",
            "training loss: 51.361846923828125\n",
            "itr: 74847\n",
            "training loss: 59.832862854003906\n",
            "itr: 74848\n",
            "training loss: 48.64300537109375\n",
            "itr: 74849\n",
            "training loss: 49.45917510986328\n",
            "itr: 74850\n",
            "training loss: 48.132022857666016\n",
            "itr: 74851\n",
            "training loss: 61.6356315612793\n",
            "itr: 74852\n",
            "training loss: 56.13285827636719\n",
            "itr: 74853\n",
            "training loss: 47.250282287597656\n",
            "itr: 74854\n",
            "training loss: 50.997772216796875\n",
            "itr: 74855\n",
            "training loss: 61.935203552246094\n",
            "itr: 74856\n",
            "training loss: 47.95318603515625\n",
            "itr: 74857\n",
            "training loss: 48.89270782470703\n",
            "itr: 74858\n",
            "training loss: 53.979942321777344\n",
            "itr: 74859\n",
            "training loss: 55.64391326904297\n",
            "itr: 74860\n",
            "training loss: 55.123756408691406\n",
            "itr: 74861\n",
            "training loss: 51.89605712890625\n",
            "itr: 74862\n",
            "training loss: 53.50450134277344\n",
            "itr: 74863\n",
            "training loss: 50.47264099121094\n",
            "itr: 74864\n",
            "training loss: 48.967140197753906\n",
            "itr: 74865\n",
            "training loss: 51.45573425292969\n",
            "itr: 74866\n",
            "training loss: 54.140869140625\n",
            "itr: 74867\n",
            "training loss: 55.344364166259766\n",
            "itr: 74868\n",
            "training loss: 48.34674072265625\n",
            "itr: 74869\n",
            "training loss: 58.298370361328125\n",
            "itr: 74870\n",
            "training loss: 53.20362091064453\n",
            "itr: 74871\n",
            "training loss: 54.43233871459961\n",
            "itr: 74872\n",
            "training loss: 55.52408218383789\n",
            "itr: 74873\n",
            "training loss: 49.647605895996094\n",
            "itr: 74874\n",
            "training loss: 43.86295700073242\n",
            "itr: 74875\n",
            "training loss: 57.388919830322266\n",
            "itr: 74876\n",
            "training loss: 56.308006286621094\n",
            "itr: 74877\n",
            "training loss: 57.67247772216797\n",
            "itr: 74878\n",
            "training loss: 58.64771270751953\n",
            "itr: 74879\n",
            "training loss: 58.26984786987305\n",
            "itr: 74880\n",
            "training loss: 55.85712432861328\n",
            "itr: 74881\n",
            "training loss: 58.35597229003906\n",
            "itr: 74882\n",
            "training loss: 50.170753479003906\n",
            "itr: 74883\n",
            "training loss: 52.113582611083984\n",
            "itr: 74884\n",
            "training loss: 47.491600036621094\n",
            "itr: 74885\n",
            "training loss: 52.3468017578125\n",
            "itr: 74886\n",
            "training loss: 52.19287109375\n",
            "itr: 74887\n",
            "training loss: 55.660552978515625\n",
            "itr: 74888\n",
            "training loss: 55.102012634277344\n",
            "itr: 74889\n",
            "training loss: 53.358375549316406\n",
            "itr: 74890\n",
            "training loss: 56.01612091064453\n",
            "itr: 74891\n",
            "training loss: 49.013519287109375\n",
            "itr: 74892\n",
            "training loss: 52.48810577392578\n",
            "itr: 74893\n",
            "training loss: 51.7684326171875\n",
            "itr: 74894\n",
            "training loss: 47.17146682739258\n",
            "itr: 74895\n",
            "training loss: 54.49916076660156\n",
            "itr: 74896\n",
            "training loss: 52.05480194091797\n",
            "itr: 74897\n",
            "training loss: 59.228302001953125\n",
            "itr: 74898\n",
            "training loss: 50.03543472290039\n",
            "itr: 74899\n",
            "training loss: 56.18529510498047\n",
            "itr: 74900\n",
            "training loss: 48.67503356933594\n",
            "itr: 74901\n",
            "training loss: 55.52153396606445\n",
            "itr: 74902\n",
            "training loss: 51.78611373901367\n",
            "itr: 74903\n",
            "training loss: 58.197818756103516\n",
            "itr: 74904\n",
            "training loss: 51.244911193847656\n",
            "itr: 74905\n",
            "training loss: 54.04392623901367\n",
            "itr: 74906\n",
            "training loss: 50.642425537109375\n",
            "itr: 74907\n",
            "training loss: 54.59893798828125\n",
            "itr: 74908\n",
            "training loss: 47.60411071777344\n",
            "itr: 74909\n",
            "training loss: 44.560394287109375\n",
            "itr: 74910\n",
            "training loss: 59.883567810058594\n",
            "itr: 74911\n",
            "training loss: 61.60783386230469\n",
            "itr: 74912\n",
            "training loss: 47.20122528076172\n",
            "itr: 74913\n",
            "training loss: 51.668251037597656\n",
            "itr: 74914\n",
            "training loss: 55.610660552978516\n",
            "itr: 74915\n",
            "training loss: 58.71099853515625\n",
            "itr: 74916\n",
            "training loss: 51.398162841796875\n",
            "itr: 74917\n",
            "training loss: 46.76322937011719\n",
            "itr: 74918\n",
            "training loss: 49.22209548950195\n",
            "itr: 74919\n",
            "training loss: 53.57613754272461\n",
            "itr: 74920\n",
            "training loss: 53.01112365722656\n",
            "itr: 74921\n",
            "training loss: 53.87881088256836\n",
            "itr: 74922\n",
            "training loss: 58.277366638183594\n",
            "itr: 74923\n",
            "training loss: 66.72061920166016\n",
            "itr: 74924\n",
            "training loss: 55.48408126831055\n",
            "itr: 74925\n",
            "training loss: 52.76799011230469\n",
            "itr: 74926\n",
            "training loss: 50.57897186279297\n",
            "itr: 74927\n",
            "training loss: 56.954349517822266\n",
            "itr: 74928\n",
            "training loss: 50.646121978759766\n",
            "itr: 74929\n",
            "training loss: 62.276283264160156\n",
            "itr: 74930\n",
            "training loss: 52.89860153198242\n",
            "itr: 74931\n",
            "training loss: 58.913856506347656\n",
            "itr: 74932\n",
            "training loss: 55.71797180175781\n",
            "itr: 74933\n",
            "training loss: 56.022552490234375\n",
            "itr: 74934\n",
            "training loss: 54.329490661621094\n",
            "itr: 74935\n",
            "training loss: 52.207637786865234\n",
            "itr: 74936\n",
            "training loss: 57.67673110961914\n",
            "itr: 74937\n",
            "training loss: 59.86980438232422\n",
            "itr: 74938\n",
            "training loss: 51.464935302734375\n",
            "itr: 74939\n",
            "training loss: 52.82749557495117\n",
            "itr: 74940\n",
            "training loss: 56.97704315185547\n",
            "itr: 74941\n",
            "training loss: 59.75782012939453\n",
            "itr: 74942\n",
            "training loss: 61.0768928527832\n",
            "itr: 74943\n",
            "training loss: 58.20245361328125\n",
            "itr: 74944\n",
            "training loss: 55.93666076660156\n",
            "itr: 74945\n",
            "training loss: 48.87094497680664\n",
            "itr: 74946\n",
            "training loss: 58.76471710205078\n",
            "itr: 74947\n",
            "training loss: 49.03941345214844\n",
            "itr: 74948\n",
            "training loss: 53.892066955566406\n",
            "itr: 74949\n",
            "training loss: 58.36207580566406\n",
            "itr: 74950\n",
            "training loss: 52.66942596435547\n",
            "itr: 74951\n",
            "training loss: 53.77220153808594\n",
            "itr: 74952\n",
            "training loss: 49.412296295166016\n",
            "itr: 74953\n",
            "training loss: 57.87782669067383\n",
            "itr: 74954\n",
            "training loss: 57.8480110168457\n",
            "itr: 74955\n",
            "training loss: 54.8836669921875\n",
            "itr: 74956\n",
            "training loss: 50.01462936401367\n",
            "itr: 74957\n",
            "training loss: 57.429771423339844\n",
            "itr: 74958\n",
            "training loss: 52.89375686645508\n",
            "itr: 74959\n",
            "training loss: 46.105552673339844\n",
            "itr: 74960\n",
            "training loss: 47.42985534667969\n",
            "itr: 74961\n",
            "training loss: 54.43706512451172\n",
            "itr: 74962\n",
            "training loss: 50.10015869140625\n",
            "itr: 74963\n",
            "training loss: 50.42894744873047\n",
            "itr: 74964\n",
            "training loss: 57.485504150390625\n",
            "itr: 74965\n",
            "training loss: 49.04353332519531\n",
            "itr: 74966\n",
            "training loss: 60.74394989013672\n",
            "itr: 74967\n",
            "training loss: 48.63043212890625\n",
            "itr: 74968\n",
            "training loss: 48.92302703857422\n",
            "itr: 74969\n",
            "training loss: 60.6541748046875\n",
            "itr: 74970\n",
            "training loss: 52.154884338378906\n",
            "itr: 74971\n",
            "training loss: 51.146934509277344\n",
            "itr: 74972\n",
            "training loss: 50.77278137207031\n",
            "itr: 74973\n",
            "training loss: 51.467132568359375\n",
            "itr: 74974\n",
            "training loss: 57.51580810546875\n",
            "itr: 74975\n",
            "training loss: 51.20030212402344\n",
            "itr: 74976\n",
            "training loss: 46.985225677490234\n",
            "itr: 74977\n",
            "training loss: 59.531150817871094\n",
            "itr: 74978\n",
            "training loss: 65.14036560058594\n",
            "itr: 74979\n",
            "training loss: 48.861061096191406\n",
            "itr: 74980\n",
            "training loss: 53.48150634765625\n",
            "itr: 74981\n",
            "training loss: 47.44674301147461\n",
            "itr: 74982\n",
            "training loss: 58.719390869140625\n",
            "itr: 74983\n",
            "training loss: 50.735252380371094\n",
            "itr: 74984\n",
            "training loss: 65.58599090576172\n",
            "itr: 74985\n",
            "training loss: 54.6376953125\n",
            "itr: 74986\n",
            "training loss: 46.26761245727539\n",
            "itr: 74987\n",
            "training loss: 51.231414794921875\n",
            "itr: 74988\n",
            "training loss: 50.66300964355469\n",
            "itr: 74989\n",
            "training loss: 54.087730407714844\n",
            "itr: 74990\n",
            "training loss: 50.311927795410156\n",
            "itr: 74991\n",
            "training loss: 59.43504333496094\n",
            "itr: 74992\n",
            "training loss: 47.475685119628906\n",
            "itr: 74993\n",
            "training loss: 56.09562683105469\n",
            "itr: 74994\n",
            "training loss: 56.29123306274414\n",
            "itr: 74995\n",
            "training loss: 49.10565948486328\n",
            "itr: 74996\n",
            "training loss: 48.18769836425781\n",
            "itr: 74997\n",
            "training loss: 56.04158020019531\n",
            "itr: 74998\n",
            "training loss: 55.77857971191406\n",
            "itr: 74999\n",
            "training loss: 47.82061767578125\n",
            "itr: 75000\n",
            "training loss: 56.063987731933594\n",
            "itr: 75001\n",
            "training loss: 60.95481491088867\n",
            "itr: 75002\n",
            "training loss: 48.37216567993164\n",
            "itr: 75003\n",
            "training loss: 53.08124542236328\n",
            "itr: 75004\n",
            "training loss: 56.859718322753906\n",
            "itr: 75005\n",
            "training loss: 51.686275482177734\n",
            "itr: 75006\n",
            "training loss: 55.631317138671875\n",
            "itr: 75007\n",
            "training loss: 50.774200439453125\n",
            "itr: 75008\n",
            "training loss: 51.97764587402344\n",
            "itr: 75009\n",
            "training loss: 51.44285583496094\n",
            "itr: 75010\n",
            "training loss: 46.182952880859375\n",
            "itr: 75011\n",
            "training loss: 60.88165283203125\n",
            "itr: 75012\n",
            "training loss: 48.21723937988281\n",
            "itr: 75013\n",
            "training loss: 50.856300354003906\n",
            "itr: 75014\n",
            "training loss: 58.54693603515625\n",
            "itr: 75015\n",
            "training loss: 53.43915557861328\n",
            "itr: 75016\n",
            "training loss: 52.27113723754883\n",
            "itr: 75017\n",
            "training loss: 56.363983154296875\n",
            "itr: 75018\n",
            "training loss: 59.111019134521484\n",
            "itr: 75019\n",
            "training loss: 48.394317626953125\n",
            "itr: 75020\n",
            "training loss: 45.65434265136719\n",
            "itr: 75021\n",
            "training loss: 60.320159912109375\n",
            "itr: 75022\n",
            "training loss: 53.5823860168457\n",
            "itr: 75023\n",
            "training loss: 67.95851135253906\n",
            "itr: 75024\n",
            "training loss: 43.89408874511719\n",
            "itr: 75025\n",
            "training loss: 71.712646484375\n",
            "itr: 75026\n",
            "training loss: 65.72247314453125\n",
            "itr: 75027\n",
            "training loss: 70.01927185058594\n",
            "itr: 75028\n",
            "training loss: 52.81647491455078\n",
            "itr: 75029\n",
            "training loss: 54.07149887084961\n",
            "itr: 75030\n",
            "training loss: 58.22309112548828\n",
            "itr: 75031\n",
            "training loss: 59.496070861816406\n",
            "itr: 75032\n",
            "training loss: 57.24043655395508\n",
            "itr: 75033\n",
            "training loss: 63.26988220214844\n",
            "itr: 75034\n",
            "training loss: 56.8487548828125\n",
            "itr: 75035\n",
            "training loss: 62.62273406982422\n",
            "itr: 75036\n",
            "training loss: 57.320743560791016\n",
            "itr: 75037\n",
            "training loss: 54.51573944091797\n",
            "itr: 75038\n",
            "training loss: 52.931854248046875\n",
            "itr: 75039\n",
            "training loss: 53.55741882324219\n",
            "itr: 75040\n",
            "training loss: 71.35057830810547\n",
            "itr: 75041\n",
            "training loss: 64.17821502685547\n",
            "itr: 75042\n",
            "training loss: 60.163787841796875\n",
            "itr: 75043\n",
            "training loss: 57.61676788330078\n",
            "itr: 75044\n",
            "training loss: 61.390621185302734\n",
            "itr: 75045\n",
            "training loss: 67.45175170898438\n",
            "itr: 75046\n",
            "training loss: 59.097450256347656\n",
            "itr: 75047\n",
            "training loss: 60.641151428222656\n",
            "itr: 75048\n",
            "training loss: 53.57237243652344\n",
            "itr: 75049\n",
            "training loss: 52.840213775634766\n",
            "itr: 75050\n",
            "training loss: 53.514320373535156\n",
            "itr: 75051\n",
            "training loss: 58.09364318847656\n",
            "itr: 75052\n",
            "training loss: 61.178306579589844\n",
            "itr: 75053\n",
            "training loss: 52.973758697509766\n",
            "itr: 75054\n",
            "training loss: 48.16252899169922\n",
            "itr: 75055\n",
            "training loss: 53.351951599121094\n",
            "itr: 75056\n",
            "training loss: 49.4322395324707\n",
            "itr: 75057\n",
            "training loss: 55.649749755859375\n",
            "itr: 75058\n",
            "training loss: 52.86696243286133\n",
            "itr: 75059\n",
            "training loss: 63.53697204589844\n",
            "itr: 75060\n",
            "training loss: 64.87203216552734\n",
            "itr: 75061\n",
            "training loss: 50.29315185546875\n",
            "itr: 75062\n",
            "training loss: 55.58256530761719\n",
            "itr: 75063\n",
            "training loss: 53.77091598510742\n",
            "itr: 75064\n",
            "training loss: 57.60078048706055\n",
            "itr: 75065\n",
            "training loss: 59.090728759765625\n",
            "itr: 75066\n",
            "training loss: 51.475303649902344\n",
            "itr: 75067\n",
            "training loss: 60.23480987548828\n",
            "itr: 75068\n",
            "training loss: 51.20783233642578\n",
            "itr: 75069\n",
            "training loss: 59.159339904785156\n",
            "itr: 75070\n",
            "training loss: 50.03355407714844\n",
            "itr: 75071\n",
            "training loss: 46.31782531738281\n",
            "itr: 75072\n",
            "training loss: 57.27825164794922\n",
            "itr: 75073\n",
            "training loss: 54.130775451660156\n",
            "itr: 75074\n",
            "training loss: 54.93659210205078\n",
            "itr: 75075\n",
            "training loss: 54.16188049316406\n",
            "itr: 75076\n",
            "training loss: 67.33694458007812\n",
            "itr: 75077\n",
            "training loss: 48.57518768310547\n",
            "itr: 75078\n",
            "training loss: 59.42737579345703\n",
            "itr: 75079\n",
            "training loss: 55.305747985839844\n",
            "itr: 75080\n",
            "training loss: 67.7854232788086\n",
            "itr: 75081\n",
            "training loss: 59.284828186035156\n",
            "itr: 75082\n",
            "training loss: 56.3167724609375\n",
            "itr: 75083\n",
            "training loss: 59.91497039794922\n",
            "itr: 75084\n",
            "training loss: 52.565773010253906\n",
            "itr: 75085\n",
            "training loss: 59.96601104736328\n",
            "itr: 75086\n",
            "training loss: 48.374610900878906\n",
            "itr: 75087\n",
            "training loss: 49.98417663574219\n",
            "itr: 75088\n",
            "training loss: 58.94985580444336\n",
            "itr: 75089\n",
            "training loss: 55.45571517944336\n",
            "itr: 75090\n",
            "training loss: 54.3417854309082\n",
            "itr: 75091\n",
            "training loss: 46.1713752746582\n",
            "itr: 75092\n",
            "training loss: 51.87765884399414\n",
            "itr: 75093\n",
            "training loss: 48.94950866699219\n",
            "itr: 75094\n",
            "training loss: 51.586700439453125\n",
            "itr: 75095\n",
            "training loss: 54.295310974121094\n",
            "itr: 75096\n",
            "training loss: 49.06789016723633\n",
            "itr: 75097\n",
            "training loss: 54.82847595214844\n",
            "itr: 75098\n",
            "training loss: 54.43740463256836\n",
            "itr: 75099\n",
            "training loss: 57.59812927246094\n",
            "itr: 75100\n",
            "training loss: 57.138126373291016\n",
            "itr: 75101\n",
            "training loss: 50.772987365722656\n",
            "itr: 75102\n",
            "training loss: 53.31650161743164\n",
            "itr: 75103\n",
            "training loss: 55.22115707397461\n",
            "itr: 75104\n",
            "training loss: 47.56315231323242\n",
            "itr: 75105\n",
            "training loss: 57.16765213012695\n",
            "itr: 75106\n",
            "training loss: 58.72310256958008\n",
            "itr: 75107\n",
            "training loss: 57.257896423339844\n",
            "itr: 75108\n",
            "training loss: 53.29966735839844\n",
            "itr: 75109\n",
            "training loss: 56.89225769042969\n",
            "itr: 75110\n",
            "training loss: 58.922637939453125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeIq0MyQM5lR",
        "colab_type": "text"
      },
      "source": [
        "## fine\n"
      ]
    }
  ]
}